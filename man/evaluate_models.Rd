% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model-evaluation.R
\name{evaluate_resampling}
\alias{evaluate_resampling}
\alias{evaluate_aic}
\alias{evaluate_models}
\alias{select_model}
\title{Tools for model evaluation}
\usage{
evaluate_resampling(
  model,
  data,
  metrics = list(yardstick::rmse),
  v = nrow(data),
  repeats = 1
)

evaluate_aic(model, data, ...)

evaluate_models(data, models, method = evaluate_resampling, ...)

select_model(data, models, method = evaluate_resampling, ...)
}
\arguments{
\item{model}{a model specified as an \code{trending_model} object, as returned by
\code{lm_model}, \code{glm_model}, \code{glm_nb_model}, \code{brms_model}; see
\href{trending_model}{\code{?trending_model}} for details}

\item{data}{a \code{data.frame} containing data (including the response variable
and all predictors) used in \code{model}}

\item{metrics}{a list of functions assessing model fit, with a similar
interface to \code{yardstick::rmse}; see \url{https://yardstick.tidymodels.org/}
for more information}

\item{v}{the number of equally sized data partitions to be used for K-fold
cross-validation; \code{v} cross-validations will be performed, each using \code{v - 1} partition as training set, and the remaining partition as testing
set. Defaults to 1, so that the method uses leave-one-out cross validation,
akin to Jackknife except that the testing set (and not the training set) is
used to compute the fit statistics.}

\item{repeats}{the number of times the random K-fold cross validation should
be repeated for; defaults to 1; larger values are likely to yield more
reliable / stable results, at the expense of computational time}

\item{...}{further arguments passed to \url{stats::AIC}}

\item{models}{a \code{list} of models specified as an \code{trending_model} object, as
returned by \code{lm_model}, \code{glm_model}, \code{glm_nb_model}, \code{brms_model}; see
\href{trending_model}{\code{?trending_model}} for details}

\item{method}{a \code{function} used to evaluate models: either
\code{evaluate_resampling} (default, better for selecting models with good
predictive power) or \code{evaluate_aic} (faster, focuses on goodness-of-fit
rather than predictive power)}
}
\description{
These functions provide tools for evaluating models, based on the goodness of
fit or on predictive power. \code{evaluate_aic} evaluates the goodness of fit of a
single model using Akaike's information criterion, measuring the deviance of
the model while penalising its complexity. \code{evaluate_resampling} uses
repeated K-fold cross-validation and the Root Mean Square Error (RMSE) of
testing sets to measure the predictive power of a single
model. \code{evaluate_aic} is faster, but \code{evaluate_resampling} is better-suited
to select best predicting models. \code{evaluate_models} uses either
\code{evaluate_aic} or \code{evaluate_resampling} to compare a series of
models. \code{select_model} does the same, but returns the 'best' model according
to the chosen method.
}
\details{
These functions wrap around existing functions from several
packages. \code{stats::AIC} is used in \code{evaluate_aic}, and \code{evaluate_resampling}
uses \code{rsample::vfold_cv} for cross-validation and \code{yardstick::rmse} to
calculate RMSE.
}
\seealso{
\url{stats::AIC} for computing AIC;
\url{rsample::vfold_cv} for cross validation;
\url{yardstick::rmse} for calculating RMSE; \code{yardstick} also
implements a range of other metrics for assessing model fit outlined at
\url{https://yardstick.tidymodels.org/};
\href{trending_model}{\code{?trending_model}} for the different ways to build
\code{trending_model} objects
}
